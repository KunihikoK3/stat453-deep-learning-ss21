{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 453: Deep Learning (Spring 2021)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "\n",
    "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat453-ss2021/  \n",
    "GitHub repository: https://github.com/rasbt/stat453-deep-learning-ss21\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vY4SK0xKAJgm"
   },
   "source": [
    "# RNN Classifier with LSTM Trained on Own Dataset (IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sc6xejhY-NzZ"
   },
   "source": [
    "Example notebook showing how to use an own CSV text dataset for training a simple RNN for sentiment classification (here: a binary classification problem with two labels, positive and negative) using LSTM (Long Short Term Memory) cells."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the [official migration guide](https://github.com/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb) is outdated. I used a combination of sources to migrate from the old torchtext api (pre 0.9.0) to the 0.14 api:\n",
    "\n",
    "- The official migration guide. Although it is outdated and legacy code has been removed, some parts are still useful. Primarily the method of building the vocabulary, ie using a `Counter` and iterating over the sentences. There is a new [`build_vocab_from_iterator`](https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator) method which does the same thing but it doesn't allow easy inspection of, for example, the 10 most common words\n",
    "\n",
    "- The [torchtext documentation](https://pytorch.org/text/stable/index.html)\n",
    "\n",
    "- The [torchdata documentation](https://pytorch.org/data/0.5/) (looking through the classes/methods in an attempt to replace to old code). Torchtext moved most of its functionality into this new package. The torchtext library is only used to build the vocabulary now\n",
    "\n",
    "- This guide has some nice parts, however, it is missing some stuff like the splitting of the data into training and test. https://medium.com/@bitdribble/migrate-torchtext-to-the-new-0-9-0-api-1ff1472b5d71\n",
    "\n",
    "- Torchtext Github issues such as https://github.com/pytorch/text/issues/711#issuecomment-1154000107 and https://github.com/pytorch/text/issues/1349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moNmVfuvnImW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neil\\anaconda3\\envs\\Py39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %load_ext watermark\n",
    "# %watermark -a 'Sebastian Raschka' -v -p torch,torchtext\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext # Not avaialable for conda-forge on windows. Just install it using pip\n",
    "import torchdata # needed as of torchtext 0.12\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSRL42Qgy8I8"
   },
   "source": [
    "## General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvW1RgfepCBq"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "DATA_DIRECTORY = 'data/Raschka/Lecture15'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQMmKUEisW4W"
   },
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells will download the IMDB movie review dataset (http://ai.stanford.edu/~amaas/data/sentiment/) for positive-negative sentiment classification in as CSV-formatted file:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On windows, just visit the below link, download and extract the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -f movie_data.csv.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the dataset looks okay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT_COLUMN_NAME</th>\n",
       "      <th>LABEL_COLUMN_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        TEXT_COLUMN_NAME  LABEL_COLUMN_NAME\n",
       "49995  OK, lets start with the best. the building. al...                  0\n",
       "49996  The British 'heritage film' industry is out of...                  0\n",
       "49997  I don't even know where to begin on this one. ...                  0\n",
       "49998  Richard Tyler is a little boy who is scared of...                  0\n",
       "49999  I waited long to watch this movie. Also becaus...                  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIRECTORY, 'movie_data.csv'))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT_COLUMN_NAME</th>\n",
       "      <th>LABEL_COLUMN_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    TEXT_COLUMN_NAME  LABEL_COLUMN_NAME\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...                  1\n",
       "1  OK... so... I really like Kris Kristofferson a...                  0\n",
       "2  ***SPOILER*** Do not read this, if you think a...                  0\n",
       "3  hi for all the people who have seen this wonde...                  1\n",
       "4  I recently bought the DVD, forgetting just how...                  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME']\n",
    "df.to_csv(os.path.join(DATA_DIRECTORY, 'movie_data.csv'), index=None)\n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_DIRECTORY, 'movie_data.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset with Torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download English vocabulary via:\n",
    "    \n",
    "- `python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GnH64XvsV8n"
   },
   "source": [
    "Load data and tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Raschka/Lecture15/movie_data.csv')\n",
    "\n",
    "dp = torchdata.datapipes.map.SequenceWrapper(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"Murder in Greenwich\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available',\n",
       "        1], dtype=object),\n",
       " array([\"OK... so... I really like Kris Kristofferson and his usual easy going delivery of lines in his movies. Age has helped him with his soft spoken low energy style and he will steal a scene effortlessly. But, Disappearance is his misstep. Holy Moly, this was a bad movie! <br /><br />I must give kudos to the cinematography and and the actors, including Kris, for trying their darndest to make sense from this goofy, confusing story! None of it made sense and Kris probably didn't understand it either and he was just going through the motions hoping someone would come up to him and tell him what it was all about! <br /><br />I don't care that everyone on this movie was doing out of love for the project, or some such nonsense... I've seen low budget movies that had a plot for goodness sake! This had none, zilcho, nada, zippo, empty of reason... a complete waste of good talent, scenery and celluloid! <br /><br />I rented this piece of garbage for a buck, and I want my money back! I want my 2 hours back I invested on this Grade F waste of my time! Don't watch this movie, or waste 1 minute of your valuable time while passing through a room where it's playing or even open up the case that is holding the DVD! Believe me, you'll thank me for the advice!\",\n",
       "        0], dtype=object),\n",
       " array(['***SPOILER*** Do not read this, if you think about watching that movie, although it would be a waste of time. (By the way: The plot is so predictable that it does not make any difference if you read this or not anyway)<br /><br />If you are wondering whether to see \"Coyote Ugly\" or not: don\\'t! It\\'s not worth either the money for the ticket or the VHS / DVD. A typical \"Chick-Feel-Good-Flick\", one could say. The plot itself is as shallow as it can be, a ridiculous and uncritical version of the American Dream. The young good-looking girl from a small town becoming a big success in New York. The few desperate attempts of giving the movie any depth fail, such as the \"tragic\" accident of the father, the \"difficulties\" of Violet\\'s relationship with her boyfriend, and so on. McNally (Director) tries to arouse the audience\\'s pity and sadness put does not have any chance to succeed in this attempt due to the bad script and the shallow acting. Especially Piper Perabo completely fails in convincing one of \"Jersey\\'s\" fear of singing in front of an audience. The only good (and quite funny thing) about \"Coyote Ugly\" is John Goodman, who represents the small ray of hope of this movie.<br /><br />I was very astonished, that Jerry Bruckheimer produced this movie. First \"Gone In 60 Seconds\" and now this... what happened to great movies like \"The Rock\" and \"Con Air\"? THAT was true Bruckheimer stuff.<br /><br />If you are looking for a superficial movie with good looking women just to have a relaxed evening, you should better go and see \"Charlie\\'s Angels\" (it\\'s much more funny, entertaining and self-ironic) instead of this flick.<br /><br />Two thumbs down (3 out of 10).',\n",
       "        0], dtype=object),\n",
       " array(['hi for all the people who have seen this wonderful movie im sure thet you would have liked it as much as i. i love the songs once you have seen the show you can sing along as though you are part of the show singing and dancing . dancing and singing. the song ONE is an all time fave musical song too and the strutters at the end with the mirror its so oh you have to watch this one',\n",
       "        1], dtype=object)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first four rows of dp\n",
    "list(dp)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def tokenize_dp(dataset):\n",
    "    text_dp, label_dp = dataset.unzip(2)\n",
    "\n",
    "    text_dp = text_dp.map(tokenizer) # split sentences into words\n",
    "    label_dp = label_dp.map(str) # convert labels to strings\n",
    "\n",
    "    return text_dp.zip(label_dp)\n",
    "\n",
    "tokenized_dp = tokenize_dp(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['In',\n",
       "   '1974',\n",
       "   ',',\n",
       "   'the',\n",
       "   'teenager',\n",
       "   'Martha',\n",
       "   'Moxley',\n",
       "   '(',\n",
       "   'Maggie',\n",
       "   'Grace',\n",
       "   ')',\n",
       "   'moves',\n",
       "   'to',\n",
       "   'the',\n",
       "   'high',\n",
       "   '-',\n",
       "   'class',\n",
       "   'area',\n",
       "   'of',\n",
       "   'Belle',\n",
       "   'Haven',\n",
       "   ',',\n",
       "   'Greenwich',\n",
       "   ',',\n",
       "   'Connecticut',\n",
       "   '.',\n",
       "   'On',\n",
       "   'the',\n",
       "   'Mischief',\n",
       "   'Night',\n",
       "   ',',\n",
       "   'eve',\n",
       "   'of',\n",
       "   'Halloween',\n",
       "   ',',\n",
       "   'she',\n",
       "   'was',\n",
       "   'murdered',\n",
       "   'in',\n",
       "   'the',\n",
       "   'backyard',\n",
       "   'of',\n",
       "   'her',\n",
       "   'house',\n",
       "   'and',\n",
       "   'her',\n",
       "   'murder',\n",
       "   'remained',\n",
       "   'unsolved',\n",
       "   '.',\n",
       "   'Twenty',\n",
       "   '-',\n",
       "   'two',\n",
       "   'years',\n",
       "   'later',\n",
       "   ',',\n",
       "   'the',\n",
       "   'writer',\n",
       "   'Mark',\n",
       "   'Fuhrman',\n",
       "   '(',\n",
       "   'Christopher',\n",
       "   'Meloni',\n",
       "   ')',\n",
       "   ',',\n",
       "   'who',\n",
       "   'is',\n",
       "   'a',\n",
       "   'former',\n",
       "   'LA',\n",
       "   'detective',\n",
       "   'that',\n",
       "   'has',\n",
       "   'fallen',\n",
       "   'in',\n",
       "   'disgrace',\n",
       "   'for',\n",
       "   'perjury',\n",
       "   'in',\n",
       "   'O.J.',\n",
       "   'Simpson',\n",
       "   'trial',\n",
       "   'and',\n",
       "   'moved',\n",
       "   'to',\n",
       "   'Idaho',\n",
       "   ',',\n",
       "   'decides',\n",
       "   'to',\n",
       "   'investigate',\n",
       "   'the',\n",
       "   'case',\n",
       "   'with',\n",
       "   'his',\n",
       "   'partner',\n",
       "   'Stephen',\n",
       "   'Weeks',\n",
       "   '(',\n",
       "   'Andrew',\n",
       "   'Mitchell',\n",
       "   ')',\n",
       "   'with',\n",
       "   'the',\n",
       "   'purpose',\n",
       "   'of',\n",
       "   'writing',\n",
       "   'a',\n",
       "   'book',\n",
       "   '.',\n",
       "   'The',\n",
       "   'locals',\n",
       "   'squirm',\n",
       "   'and',\n",
       "   'do',\n",
       "   'not',\n",
       "   'welcome',\n",
       "   'them',\n",
       "   ',',\n",
       "   'but',\n",
       "   'with',\n",
       "   'the',\n",
       "   'support',\n",
       "   'of',\n",
       "   'the',\n",
       "   'retired',\n",
       "   'detective',\n",
       "   'Steve',\n",
       "   'Carroll',\n",
       "   '(',\n",
       "   'Robert',\n",
       "   'Forster',\n",
       "   ')',\n",
       "   'that',\n",
       "   'was',\n",
       "   'in',\n",
       "   'charge',\n",
       "   'of',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   'in',\n",
       "   'the',\n",
       "   '70',\n",
       "   \"'s\",\n",
       "   ',',\n",
       "   'they',\n",
       "   'discover',\n",
       "   'the',\n",
       "   'criminal',\n",
       "   'and',\n",
       "   'a',\n",
       "   'net',\n",
       "   'of',\n",
       "   'power',\n",
       "   'and',\n",
       "   'money',\n",
       "   'to',\n",
       "   'cover',\n",
       "   'the',\n",
       "   'murder.<br',\n",
       "   '/><br',\n",
       "   '/>\"Murder',\n",
       "   'in',\n",
       "   'Greenwich',\n",
       "   '\"',\n",
       "   'is',\n",
       "   'a',\n",
       "   'good',\n",
       "   'TV',\n",
       "   'movie',\n",
       "   ',',\n",
       "   'with',\n",
       "   'the',\n",
       "   'true',\n",
       "   'story',\n",
       "   'of',\n",
       "   'a',\n",
       "   'murder',\n",
       "   'of',\n",
       "   'a',\n",
       "   'fifteen',\n",
       "   'years',\n",
       "   'old',\n",
       "   'girl',\n",
       "   'that',\n",
       "   'was',\n",
       "   'committed',\n",
       "   'by',\n",
       "   'a',\n",
       "   'wealthy',\n",
       "   'teenager',\n",
       "   'whose',\n",
       "   'mother',\n",
       "   'was',\n",
       "   'a',\n",
       "   'Kennedy',\n",
       "   '.',\n",
       "   'The',\n",
       "   'powerful',\n",
       "   'and',\n",
       "   'rich',\n",
       "   'family',\n",
       "   'used',\n",
       "   'their',\n",
       "   'influence',\n",
       "   'to',\n",
       "   'cover',\n",
       "   'the',\n",
       "   'murder',\n",
       "   'for',\n",
       "   'more',\n",
       "   'than',\n",
       "   'twenty',\n",
       "   'years',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'a',\n",
       "   'snoopy',\n",
       "   'detective',\n",
       "   'and',\n",
       "   'convicted',\n",
       "   'perjurer',\n",
       "   'in',\n",
       "   'disgrace',\n",
       "   'was',\n",
       "   'able',\n",
       "   'to',\n",
       "   'disclose',\n",
       "   'how',\n",
       "   'the',\n",
       "   'hideous',\n",
       "   'crime',\n",
       "   'was',\n",
       "   'committed',\n",
       "   '.',\n",
       "   'The',\n",
       "   'screenplay',\n",
       "   'shows',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   'of',\n",
       "   'Mark',\n",
       "   'and',\n",
       "   'the',\n",
       "   'last',\n",
       "   'days',\n",
       "   'of',\n",
       "   'Martha',\n",
       "   'in',\n",
       "   'parallel',\n",
       "   ',',\n",
       "   'but',\n",
       "   'there',\n",
       "   'is',\n",
       "   'a',\n",
       "   'lack',\n",
       "   'of',\n",
       "   'the',\n",
       "   'emotion',\n",
       "   'in',\n",
       "   'the',\n",
       "   'dramatization',\n",
       "   '.',\n",
       "   'My',\n",
       "   'vote',\n",
       "   'is',\n",
       "   'seven.<br',\n",
       "   '/><br',\n",
       "   '/>Title',\n",
       "   '(',\n",
       "   'Brazil',\n",
       "   '):',\n",
       "   'Not',\n",
       "   'Available'],\n",
       "  '1'),\n",
       " (['OK',\n",
       "   '...',\n",
       "   'so',\n",
       "   '...',\n",
       "   'I',\n",
       "   'really',\n",
       "   'like',\n",
       "   'Kris',\n",
       "   'Kristofferson',\n",
       "   'and',\n",
       "   'his',\n",
       "   'usual',\n",
       "   'easy',\n",
       "   'going',\n",
       "   'delivery',\n",
       "   'of',\n",
       "   'lines',\n",
       "   'in',\n",
       "   'his',\n",
       "   'movies',\n",
       "   '.',\n",
       "   'Age',\n",
       "   'has',\n",
       "   'helped',\n",
       "   'him',\n",
       "   'with',\n",
       "   'his',\n",
       "   'soft',\n",
       "   'spoken',\n",
       "   'low',\n",
       "   'energy',\n",
       "   'style',\n",
       "   'and',\n",
       "   'he',\n",
       "   'will',\n",
       "   'steal',\n",
       "   'a',\n",
       "   'scene',\n",
       "   'effortlessly',\n",
       "   '.',\n",
       "   'But',\n",
       "   ',',\n",
       "   'Disappearance',\n",
       "   'is',\n",
       "   'his',\n",
       "   'misstep',\n",
       "   '.',\n",
       "   'Holy',\n",
       "   'Moly',\n",
       "   ',',\n",
       "   'this',\n",
       "   'was',\n",
       "   'a',\n",
       "   'bad',\n",
       "   'movie',\n",
       "   '!',\n",
       "   '<',\n",
       "   'br',\n",
       "   '/><br',\n",
       "   '/>I',\n",
       "   'must',\n",
       "   'give',\n",
       "   'kudos',\n",
       "   'to',\n",
       "   'the',\n",
       "   'cinematography',\n",
       "   'and',\n",
       "   'and',\n",
       "   'the',\n",
       "   'actors',\n",
       "   ',',\n",
       "   'including',\n",
       "   'Kris',\n",
       "   ',',\n",
       "   'for',\n",
       "   'trying',\n",
       "   'their',\n",
       "   'darndest',\n",
       "   'to',\n",
       "   'make',\n",
       "   'sense',\n",
       "   'from',\n",
       "   'this',\n",
       "   'goofy',\n",
       "   ',',\n",
       "   'confusing',\n",
       "   'story',\n",
       "   '!',\n",
       "   'None',\n",
       "   'of',\n",
       "   'it',\n",
       "   'made',\n",
       "   'sense',\n",
       "   'and',\n",
       "   'Kris',\n",
       "   'probably',\n",
       "   'did',\n",
       "   \"n't\",\n",
       "   'understand',\n",
       "   'it',\n",
       "   'either',\n",
       "   'and',\n",
       "   'he',\n",
       "   'was',\n",
       "   'just',\n",
       "   'going',\n",
       "   'through',\n",
       "   'the',\n",
       "   'motions',\n",
       "   'hoping',\n",
       "   'someone',\n",
       "   'would',\n",
       "   'come',\n",
       "   'up',\n",
       "   'to',\n",
       "   'him',\n",
       "   'and',\n",
       "   'tell',\n",
       "   'him',\n",
       "   'what',\n",
       "   'it',\n",
       "   'was',\n",
       "   'all',\n",
       "   'about',\n",
       "   '!',\n",
       "   '<',\n",
       "   'br',\n",
       "   '/><br',\n",
       "   '/>I',\n",
       "   'do',\n",
       "   \"n't\",\n",
       "   'care',\n",
       "   'that',\n",
       "   'everyone',\n",
       "   'on',\n",
       "   'this',\n",
       "   'movie',\n",
       "   'was',\n",
       "   'doing',\n",
       "   'out',\n",
       "   'of',\n",
       "   'love',\n",
       "   'for',\n",
       "   'the',\n",
       "   'project',\n",
       "   ',',\n",
       "   'or',\n",
       "   'some',\n",
       "   'such',\n",
       "   'nonsense',\n",
       "   '...',\n",
       "   'I',\n",
       "   \"'ve\",\n",
       "   'seen',\n",
       "   'low',\n",
       "   'budget',\n",
       "   'movies',\n",
       "   'that',\n",
       "   'had',\n",
       "   'a',\n",
       "   'plot',\n",
       "   'for',\n",
       "   'goodness',\n",
       "   'sake',\n",
       "   '!',\n",
       "   'This',\n",
       "   'had',\n",
       "   'none',\n",
       "   ',',\n",
       "   'zilcho',\n",
       "   ',',\n",
       "   'nada',\n",
       "   ',',\n",
       "   'zippo',\n",
       "   ',',\n",
       "   'empty',\n",
       "   'of',\n",
       "   'reason',\n",
       "   '...',\n",
       "   'a',\n",
       "   'complete',\n",
       "   'waste',\n",
       "   'of',\n",
       "   'good',\n",
       "   'talent',\n",
       "   ',',\n",
       "   'scenery',\n",
       "   'and',\n",
       "   'celluloid',\n",
       "   '!',\n",
       "   '<',\n",
       "   'br',\n",
       "   '/><br',\n",
       "   '/>I',\n",
       "   'rented',\n",
       "   'this',\n",
       "   'piece',\n",
       "   'of',\n",
       "   'garbage',\n",
       "   'for',\n",
       "   'a',\n",
       "   'buck',\n",
       "   ',',\n",
       "   'and',\n",
       "   'I',\n",
       "   'want',\n",
       "   'my',\n",
       "   'money',\n",
       "   'back',\n",
       "   '!',\n",
       "   'I',\n",
       "   'want',\n",
       "   'my',\n",
       "   '2',\n",
       "   'hours',\n",
       "   'back',\n",
       "   'I',\n",
       "   'invested',\n",
       "   'on',\n",
       "   'this',\n",
       "   'Grade',\n",
       "   'F',\n",
       "   'waste',\n",
       "   'of',\n",
       "   'my',\n",
       "   'time',\n",
       "   '!',\n",
       "   'Do',\n",
       "   \"n't\",\n",
       "   'watch',\n",
       "   'this',\n",
       "   'movie',\n",
       "   ',',\n",
       "   'or',\n",
       "   'waste',\n",
       "   '1',\n",
       "   'minute',\n",
       "   'of',\n",
       "   'your',\n",
       "   'valuable',\n",
       "   'time',\n",
       "   'while',\n",
       "   'passing',\n",
       "   'through',\n",
       "   'a',\n",
       "   'room',\n",
       "   'where',\n",
       "   'it',\n",
       "   \"'s\",\n",
       "   'playing',\n",
       "   'or',\n",
       "   'even',\n",
       "   'open',\n",
       "   'up',\n",
       "   'the',\n",
       "   'case',\n",
       "   'that',\n",
       "   'is',\n",
       "   'holding',\n",
       "   'the',\n",
       "   'DVD',\n",
       "   '!',\n",
       "   'Believe',\n",
       "   'me',\n",
       "   ',',\n",
       "   'you',\n",
       "   \"'ll\",\n",
       "   'thank',\n",
       "   'me',\n",
       "   'for',\n",
       "   'the',\n",
       "   'advice',\n",
       "   '!'],\n",
       "  '0'),\n",
       " (['*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'SPOILER',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Do',\n",
       "   'not',\n",
       "   'read',\n",
       "   'this',\n",
       "   ',',\n",
       "   'if',\n",
       "   'you',\n",
       "   'think',\n",
       "   'about',\n",
       "   'watching',\n",
       "   'that',\n",
       "   'movie',\n",
       "   ',',\n",
       "   'although',\n",
       "   'it',\n",
       "   'would',\n",
       "   'be',\n",
       "   'a',\n",
       "   'waste',\n",
       "   'of',\n",
       "   'time',\n",
       "   '.',\n",
       "   '(',\n",
       "   'By',\n",
       "   'the',\n",
       "   'way',\n",
       "   ':',\n",
       "   'The',\n",
       "   'plot',\n",
       "   'is',\n",
       "   'so',\n",
       "   'predictable',\n",
       "   'that',\n",
       "   'it',\n",
       "   'does',\n",
       "   'not',\n",
       "   'make',\n",
       "   'any',\n",
       "   'difference',\n",
       "   'if',\n",
       "   'you',\n",
       "   'read',\n",
       "   'this',\n",
       "   'or',\n",
       "   'not',\n",
       "   'anyway)<br',\n",
       "   '/><br',\n",
       "   '/>If',\n",
       "   'you',\n",
       "   'are',\n",
       "   'wondering',\n",
       "   'whether',\n",
       "   'to',\n",
       "   'see',\n",
       "   '\"',\n",
       "   'Coyote',\n",
       "   'Ugly',\n",
       "   '\"',\n",
       "   'or',\n",
       "   'not',\n",
       "   ':',\n",
       "   'do',\n",
       "   \"n't\",\n",
       "   '!',\n",
       "   'It',\n",
       "   \"'s\",\n",
       "   'not',\n",
       "   'worth',\n",
       "   'either',\n",
       "   'the',\n",
       "   'money',\n",
       "   'for',\n",
       "   'the',\n",
       "   'ticket',\n",
       "   'or',\n",
       "   'the',\n",
       "   'VHS',\n",
       "   '/',\n",
       "   'DVD',\n",
       "   '.',\n",
       "   'A',\n",
       "   'typical',\n",
       "   '\"',\n",
       "   'Chick',\n",
       "   '-',\n",
       "   'Feel',\n",
       "   '-',\n",
       "   'Good',\n",
       "   '-',\n",
       "   'Flick',\n",
       "   '\"',\n",
       "   ',',\n",
       "   'one',\n",
       "   'could',\n",
       "   'say',\n",
       "   '.',\n",
       "   'The',\n",
       "   'plot',\n",
       "   'itself',\n",
       "   'is',\n",
       "   'as',\n",
       "   'shallow',\n",
       "   'as',\n",
       "   'it',\n",
       "   'can',\n",
       "   'be',\n",
       "   ',',\n",
       "   'a',\n",
       "   'ridiculous',\n",
       "   'and',\n",
       "   'uncritical',\n",
       "   'version',\n",
       "   'of',\n",
       "   'the',\n",
       "   'American',\n",
       "   'Dream',\n",
       "   '.',\n",
       "   'The',\n",
       "   'young',\n",
       "   'good',\n",
       "   '-',\n",
       "   'looking',\n",
       "   'girl',\n",
       "   'from',\n",
       "   'a',\n",
       "   'small',\n",
       "   'town',\n",
       "   'becoming',\n",
       "   'a',\n",
       "   'big',\n",
       "   'success',\n",
       "   'in',\n",
       "   'New',\n",
       "   'York',\n",
       "   '.',\n",
       "   'The',\n",
       "   'few',\n",
       "   'desperate',\n",
       "   'attempts',\n",
       "   'of',\n",
       "   'giving',\n",
       "   'the',\n",
       "   'movie',\n",
       "   'any',\n",
       "   'depth',\n",
       "   'fail',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   '\"',\n",
       "   'tragic',\n",
       "   '\"',\n",
       "   'accident',\n",
       "   'of',\n",
       "   'the',\n",
       "   'father',\n",
       "   ',',\n",
       "   'the',\n",
       "   '\"',\n",
       "   'difficulties',\n",
       "   '\"',\n",
       "   'of',\n",
       "   'Violet',\n",
       "   \"'s\",\n",
       "   'relationship',\n",
       "   'with',\n",
       "   'her',\n",
       "   'boyfriend',\n",
       "   ',',\n",
       "   'and',\n",
       "   'so',\n",
       "   'on',\n",
       "   '.',\n",
       "   'McNally',\n",
       "   '(',\n",
       "   'Director',\n",
       "   ')',\n",
       "   'tries',\n",
       "   'to',\n",
       "   'arouse',\n",
       "   'the',\n",
       "   'audience',\n",
       "   \"'s\",\n",
       "   'pity',\n",
       "   'and',\n",
       "   'sadness',\n",
       "   'put',\n",
       "   'does',\n",
       "   'not',\n",
       "   'have',\n",
       "   'any',\n",
       "   'chance',\n",
       "   'to',\n",
       "   'succeed',\n",
       "   'in',\n",
       "   'this',\n",
       "   'attempt',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'bad',\n",
       "   'script',\n",
       "   'and',\n",
       "   'the',\n",
       "   'shallow',\n",
       "   'acting',\n",
       "   '.',\n",
       "   'Especially',\n",
       "   'Piper',\n",
       "   'Perabo',\n",
       "   'completely',\n",
       "   'fails',\n",
       "   'in',\n",
       "   'convincing',\n",
       "   'one',\n",
       "   'of',\n",
       "   '\"',\n",
       "   'Jersey',\n",
       "   \"'s\",\n",
       "   '\"',\n",
       "   'fear',\n",
       "   'of',\n",
       "   'singing',\n",
       "   'in',\n",
       "   'front',\n",
       "   'of',\n",
       "   'an',\n",
       "   'audience',\n",
       "   '.',\n",
       "   'The',\n",
       "   'only',\n",
       "   'good',\n",
       "   '(',\n",
       "   'and',\n",
       "   'quite',\n",
       "   'funny',\n",
       "   'thing',\n",
       "   ')',\n",
       "   'about',\n",
       "   '\"',\n",
       "   'Coyote',\n",
       "   'Ugly',\n",
       "   '\"',\n",
       "   'is',\n",
       "   'John',\n",
       "   'Goodman',\n",
       "   ',',\n",
       "   'who',\n",
       "   'represents',\n",
       "   'the',\n",
       "   'small',\n",
       "   'ray',\n",
       "   'of',\n",
       "   'hope',\n",
       "   'of',\n",
       "   'this',\n",
       "   'movie.<br',\n",
       "   '/><br',\n",
       "   '/>I',\n",
       "   'was',\n",
       "   'very',\n",
       "   'astonished',\n",
       "   ',',\n",
       "   'that',\n",
       "   'Jerry',\n",
       "   'Bruckheimer',\n",
       "   'produced',\n",
       "   'this',\n",
       "   'movie',\n",
       "   '.',\n",
       "   'First',\n",
       "   '\"',\n",
       "   'Gone',\n",
       "   'In',\n",
       "   '60',\n",
       "   'Seconds',\n",
       "   '\"',\n",
       "   'and',\n",
       "   'now',\n",
       "   'this',\n",
       "   '...',\n",
       "   'what',\n",
       "   'happened',\n",
       "   'to',\n",
       "   'great',\n",
       "   'movies',\n",
       "   'like',\n",
       "   '\"',\n",
       "   'The',\n",
       "   'Rock',\n",
       "   '\"',\n",
       "   'and',\n",
       "   '\"',\n",
       "   'Con',\n",
       "   'Air',\n",
       "   '\"',\n",
       "   '?',\n",
       "   'THAT',\n",
       "   'was',\n",
       "   'true',\n",
       "   'Bruckheimer',\n",
       "   'stuff.<br',\n",
       "   '/><br',\n",
       "   '/>If',\n",
       "   'you',\n",
       "   'are',\n",
       "   'looking',\n",
       "   'for',\n",
       "   'a',\n",
       "   'superficial',\n",
       "   'movie',\n",
       "   'with',\n",
       "   'good',\n",
       "   'looking',\n",
       "   'women',\n",
       "   'just',\n",
       "   'to',\n",
       "   'have',\n",
       "   'a',\n",
       "   'relaxed',\n",
       "   'evening',\n",
       "   ',',\n",
       "   'you',\n",
       "   'should',\n",
       "   'better',\n",
       "   'go',\n",
       "   'and',\n",
       "   'see',\n",
       "   '\"',\n",
       "   'Charlie',\n",
       "   \"'s\",\n",
       "   'Angels',\n",
       "   '\"',\n",
       "   '(',\n",
       "   'it',\n",
       "   \"'s\",\n",
       "   'much',\n",
       "   'more',\n",
       "   'funny',\n",
       "   ',',\n",
       "   'entertaining',\n",
       "   'and',\n",
       "   'self',\n",
       "   '-',\n",
       "   'ironic',\n",
       "   ')',\n",
       "   'instead',\n",
       "   'of',\n",
       "   'this',\n",
       "   'flick.<br',\n",
       "   '/><br',\n",
       "   '/>Two',\n",
       "   'thumbs',\n",
       "   'down',\n",
       "   '(',\n",
       "   '3',\n",
       "   'out',\n",
       "   'of',\n",
       "   '10',\n",
       "   ')',\n",
       "   '.'],\n",
       "  '0'),\n",
       " (['hi',\n",
       "   'for',\n",
       "   'all',\n",
       "   'the',\n",
       "   'people',\n",
       "   'who',\n",
       "   'have',\n",
       "   'seen',\n",
       "   'this',\n",
       "   'wonderful',\n",
       "   'movie',\n",
       "   'i',\n",
       "   'm',\n",
       "   'sure',\n",
       "   'thet',\n",
       "   'you',\n",
       "   'would',\n",
       "   'have',\n",
       "   'liked',\n",
       "   'it',\n",
       "   'as',\n",
       "   'much',\n",
       "   'as',\n",
       "   'i.',\n",
       "   'i',\n",
       "   'love',\n",
       "   'the',\n",
       "   'songs',\n",
       "   'once',\n",
       "   'you',\n",
       "   'have',\n",
       "   'seen',\n",
       "   'the',\n",
       "   'show',\n",
       "   'you',\n",
       "   'can',\n",
       "   'sing',\n",
       "   'along',\n",
       "   'as',\n",
       "   'though',\n",
       "   'you',\n",
       "   'are',\n",
       "   'part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'show',\n",
       "   'singing',\n",
       "   'and',\n",
       "   'dancing',\n",
       "   '.',\n",
       "   'dancing',\n",
       "   'and',\n",
       "   'singing',\n",
       "   '.',\n",
       "   'the',\n",
       "   'song',\n",
       "   'ONE',\n",
       "   'is',\n",
       "   'an',\n",
       "   'all',\n",
       "   'time',\n",
       "   'fave',\n",
       "   'musical',\n",
       "   'song',\n",
       "   'too',\n",
       "   'and',\n",
       "   'the',\n",
       "   'strutters',\n",
       "   'at',\n",
       "   'the',\n",
       "   'end',\n",
       "   'with',\n",
       "   'the',\n",
       "   'mirror',\n",
       "   'its',\n",
       "   'so',\n",
       "   'oh',\n",
       "   'you',\n",
       "   'have',\n",
       "   'to',\n",
       "   'watch',\n",
       "   'this',\n",
       "   'one'],\n",
       "  '1')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first four rows of tokenized_dp\n",
    "list(tokenized_dp)[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Train/Validation/Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, validation, and test partitions.\n",
    "- training: used to train the model\n",
    "- validation: used to test the model during training (I dont know what the difference between this and test is).\n",
    "- test: used to test the model after training\n",
    "\n",
    "We use the `torch.utils.data.random_split` method instead of [`torchdata.datapipes.iter.RandomSplitter`](https://pytorch.org/data/main/generated/torchdata.datapipes.iter.RandomSplitter.html#torchdata.datapipes.iter.RandomSplitter) because the RandomSplitter does not use the fact that the tokenized_dp is indexable (ie you can randomly access any of the entries). We could use it, but it causes many warnings to occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WZ_4jiHVnMxN",
    "outputId": "dfa51c04-4845-44c3-f50b-d36d41f132b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 40000\n",
      "Num Test: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data_1, test_data = torch.utils.data.random_split(\n",
    "    tokenized_dp,\n",
    "    [0.8, 0.2],\n",
    "    generator = torch.Generator().manual_seed(RANDOM_SEED)\n",
    ")\n",
    "\n",
    "print(f'Num Train: {len(train_data_1)}')\n",
    "print(f'Num Test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 34000\n",
      "Num Validation: 6000\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = torch.utils.data.random_split(\n",
    "    train_data_1,\n",
    "    [0.85, 0.15],\n",
    "    generator = torch.Generator().manual_seed(RANDOM_SEED)\n",
    ")\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Validation: {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['The', 'GREAT', 'NEWS', 'is', 'that', 'this', 'film', 'is', 'now', 'AVAILABLE', 'on', 'DVD', 'from', 'http://treasureflix.com', 'for', 'all', 'those', 'who', 'wish', 'to', 'own', 'it', 'as', 'well', 'as', 'on', 'video', '.', 'This', 'is', 'good', 'news', 'as', 'it', 'is', 'one', 'of', 'my', 'favourite', 'films!<br', '/><br', '/>I', 'watched', 'this', 'film', 'for', 'the', 'first', 'time', 'in', 'the', '80s', 'and', 'it', 'is', 'compulsory', 'holiday', 'viewing', '.', 'Living', 'in', 'the', 'small', 'market', 'town', 'called', 'Tewkesbury', ',', 'picturesque', 'and', 'with', 'its', 'own', 'traditions', ',', 'of', 'reenactments', ',', 'and', 'traditions', 'we', 'are', 'also', 'a', 'cosy', 'tight', 'community', '.', 'We', 'are', 'now', 'also', 'faced', 'with', 'large', 'housing', 'developments', 'which', 'threaten', 'to', 'destroy', 'the', 'Community', 'and', 'you', 'can', 'see', 'why', 'I', 'love', 'this', 'film', 'First', 'of', 'all', '-', 'and', 'most', 'important', ',', 'there', 'are', 'LASHINGS', 'of', 'snow', '!', '!', '!', '!', 'Then', 'there', 'is', 'the', 'lovely', 'legend', 'of', 'the', 'Christmas', 'Tree', 'and', 'also', 'the', 'Christian', 'denouement', 'as', 'all', 'the', 'community', 'cough', 'up', 'the', 'money', 'to', 'help', 'the', 'destitute', 'farmer', 'save', 'his', 'farm', 'and', 'stay', 'in', 'the', 'community', '.', 'The', 'evil', 'developers', '-', 'only', 'after', 'the', 'money', 'are', 'sent', 'packing', 'as', 'the', 'whole', 'town', 'pledge', 'their', 'money', 'to', 'help', 'protect', 'what', 'they', 'have', ',', 'which', 'is', 'very', 'special', '.', 'I', 'love', 'the', 'way', 'the', 'whole', 'community', 'send', 'their', 'message', 'to', 'Santa', 'via', 'the', 'post', 'office', 'which', 'is', 'misunderstood', 'by', 'the', 'hero', '.', 'He', 'and', 'his', 'daughter', 'have', 'a', 'long', 'journey', 'to', 'make', 'after', 'the', 'death', 'of', 'the', 'wife', 'and', 'mother', 'of', 'the', 'family', '.', '(', 'There', 'is', 'a', 'likely', 'candidate', 'for', 'this)-even', 'a', 'sleigh', 'ride', 'and', 'most', 'heartwarming', 'of', 'all', 'is', 'that', 'the', 'taxi', 'driver', ',', 'whose', 'engine', 'is', 'broken', 'is', 'mysteriously', 'given', 'a', 'new', 'one', 'on', 'Christmas', 'morning', 'and', 'no', '-', 'one', 'had', 'engineered', 'it', '!', 'There', 'is', 'a', 'lovely', 'moment', 'where', 'Denver', 'sings', 'a', 'lullaby', 'and', 'an', 'exciting', 'search', '.', 'Great', 'gentle', 'film', 'for', 'everything', 'Christmas', 'is', 'really', 'about', '.'], '1')\n"
     ]
    }
   ],
   "source": [
    "for example in train_data:\n",
    "    print(example)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-TBwKWPslPa"
   },
   "source": [
    "Build the vocabulary. We\n",
    "\n",
    "1. Get the number of occurrences of each word/label\n",
    "2. Construct a vocab object with the most common words and with special tokens `<unk>` and `<pad>`\n",
    "3. Set `<unk>` to be the fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "text_counter = Counter()\n",
    "label_counter = Counter()\n",
    "\n",
    "for line, label in train_data:\n",
    "    text_counter.update(line)\n",
    "    label_counter.update([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocab = torchtext.vocab.vocab(\n",
    "    OrderedDict(text_counter.most_common(VOCABULARY_SIZE)), # choose most common\n",
    "    specials=('<unk>', '<pad>'), # extra words added to the vocab\n",
    ")\n",
    "text_vocab.set_default_index(text_vocab['<unk>']) # we need to manually set that <unk> is the default\n",
    "\n",
    "label_vocab = torchtext.vocab.vocab(OrderedDict(label_counter)) # label_vocab does not have specials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "e8uNrjdtn4A8",
    "outputId": "6cf499d7-7722-4da0-8576-ee0f218cc6e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocabulary size: {len(text_vocab)}')\n",
    "print(f'Number of classes: {len(label_vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 20,002 not 20,000 because of the `<unk>` and `<pad>` tokens\n",
    "- PyTorch RNNs can deal with arbitrary lengths due to dynamic graphs, but padding is necessary for padding sequences to the same length in a given minibatch so we can store those in an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at most common words:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 390911), (',', 368217), ('.', 318439), ('and', 210500), ('a', 210490), ('of', 194463), ('to', 179740), ('is', 146136), ('in', 118736), ('I', 105440), ('it', 103564), ('that', 94370), ('\"', 85796), (\"'s\", 83204), ('this', 81393), ('-', 71103), ('/><br', 68674), ('was', 67783), ('movie', 57572), ('as', 57538)]\n"
     ]
    }
   ],
   "source": [
    "print(text_counter.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokens corresponding to the first 10 indices (0, 1, ..., 9):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(text_vocab.get_itos()[:10]) # itos = integer-to-string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting a string to an integer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(text_vocab['the']) # stoi = string-to-integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class labels:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '0': 1}\n"
     ]
    }
   ],
   "source": [
    "print(label_vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class label count:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1': 17029, '0': 16971})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIQ_zfKLwjKm"
   },
   "source": [
    "## Define Data Loaders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we define the dataloaders, let us get our datasets in their proper form. That is, an iterable of (list of word_index, label_index) pairs. We also, set `in_memory_cache()` to cache the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_indexed(dataset):\n",
    "    dataset = torchdata.datapipes.map.SequenceWrapper(dataset)\n",
    "    text_dataset, label_dataset = dataset.unzip(2)\n",
    "\n",
    "    text_dataset = text_dataset.map(text_vocab).map(torch.tensor)\n",
    "    label_dataset = label_dataset.map(label_vocab.__getitem__).map(torch.tensor)\n",
    "\n",
    "    dataset = text_dataset.zip(label_dataset)\n",
    "\n",
    "    return dataset.in_memory_cache()\n",
    "\n",
    "train_idata = convert_to_indexed(train_data)\n",
    "valid_idata = convert_to_indexed(valid_data)\n",
    "test_idata = convert_to_indexed(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate into batches. The `collate_batch function` takes a list of (list of text_index, label_index) pairs and\n",
    "\n",
    "1. Splits it into two lists\n",
    "2. Converts the label_list to a single vector\n",
    "3. Converts text_list into a 2d array (matrix) with necessary padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code which attempts to produce buckets with sentences of similar length\n",
    "# def sort_bucket(bucket):\n",
    "#     sorted(bucket, key=lambda x: len(x[0]))\n",
    "# def batch_data(dataset):\n",
    "#     dataset = torchdata.datapipes.iter.BucketBatcher(\n",
    "#         torchdata.datapipes.iter.IterableWrapper(dataset),\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         sort_key=sort_bucket\n",
    "#     )\n",
    "#     return dataset\n",
    "\n",
    "# def collate_batch(batch):\n",
    "#     text_list = [text for text, label in batch]\n",
    "#     label_list = [label for text, label in batch]\n",
    "\n",
    "#     padding_value = text_vocab.get_stoi()['<pad>']\n",
    "\n",
    "#     label_list = torch.tensor(label_list)\n",
    "#     text_list = torch.nn.utils.rnn.pad_sequence(text_list, padding_value=padding_value)\n",
    "#     return text_list, label_list\n",
    "\n",
    "# train_loader = batch_data(train_idata).map(collate_batch)\n",
    "# valid_loader = batch_data(valid_idata).map(collate_batch)\n",
    "# train_loader = batch_data(train_idata).map(collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7JiHR1stHNF"
   },
   "outputs": [],
   "source": [
    "\n",
    "def collate_batch(batch):\n",
    "    text_list = [text for text, label in batch]\n",
    "    label_list = [label for text, label in batch]\n",
    "\n",
    "    padding_value = text_vocab.get_stoi()['<pad>']\n",
    "\n",
    "    label_list = torch.tensor(label_list)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, padding_value=padding_value)\n",
    "    return text_list, label_list\n",
    "\n",
    "\n",
    "\n",
    "# rain_iter = IMDB(split=train) \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_idata,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    collate_fn=collate_batch)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "                    valid_idata, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    collate_fn=collate_batch)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    test_idata, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0pT_dMRvicQ"
   },
   "source": [
    "Testing the iterators (note that the number of rows depends on the longest document in the respective batch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "y8SP_FccutT0",
    "outputId": "fe33763a-4560-4dee-adee-31cc6c48b0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([1052, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Valid:\n",
      "Text matrix size: torch.Size([1193, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([1120, 128])\n",
      "Target vector size: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch[0].size()}')\n",
    "    print(f'Target vector size: {batch[1].size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nValid:')\n",
    "for batch in valid_loader:\n",
    "    print(f'Text matrix size: {batch[0].size()}')\n",
    "    print(f'Target vector size: {batch[1].size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:')\n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch[0].size()}')\n",
    "    print(f'Target vector size: {batch[1].size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_grdW3pxCzz"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQIUm5EjxFNa"
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        #self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                        hidden_dim,\n",
    "        #                        nonlinearity='relu')\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim)        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, text):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "\n",
    "        hidden.squeeze_(0)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "        \n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ik3NF3faxFmZ"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(input_dim=len(text_vocab),\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=NUM_CLASSES # could use 1 for binary classification\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lv9Ny9di6VcI"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T5t1Afn4xO11"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1836
    },
    "colab_type": "code",
    "id": "EABZM8Vo0ilB",
    "outputId": "5d45e293-9909-4588-e793-8dfaf72e5c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/266 | Loss: 0.6902\n",
      "Epoch: 001/015 | Batch 050/266 | Loss: 0.6930\n",
      "Epoch: 001/015 | Batch 100/266 | Loss: 0.6940\n",
      "Epoch: 001/015 | Batch 150/266 | Loss: 0.6911\n",
      "Epoch: 001/015 | Batch 200/266 | Loss: 0.7045\n",
      "Epoch: 001/015 | Batch 250/266 | Loss: 0.6944\n",
      "training accuracy: 50.09%\n",
      "valid accuracy: 50.18%\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 002/015 | Batch 000/266 | Loss: 0.6929\n",
      "Epoch: 002/015 | Batch 050/266 | Loss: 0.6904\n",
      "Epoch: 002/015 | Batch 100/266 | Loss: 0.6928\n",
      "Epoch: 002/015 | Batch 150/266 | Loss: 0.6883\n",
      "Epoch: 002/015 | Batch 200/266 | Loss: 0.7093\n",
      "Epoch: 002/015 | Batch 250/266 | Loss: 0.6935\n",
      "training accuracy: 50.23%\n",
      "valid accuracy: 50.15%\n",
      "Time elapsed: 1.43 min\n",
      "Epoch: 003/015 | Batch 000/266 | Loss: 0.6928\n",
      "Epoch: 003/015 | Batch 050/266 | Loss: 0.6867\n",
      "Epoch: 003/015 | Batch 100/266 | Loss: 0.6948\n",
      "Epoch: 003/015 | Batch 150/266 | Loss: 0.6881\n",
      "Epoch: 003/015 | Batch 200/266 | Loss: 0.6920\n",
      "Epoch: 003/015 | Batch 250/266 | Loss: 0.6898\n",
      "training accuracy: 50.28%\n",
      "valid accuracy: 50.15%\n",
      "Time elapsed: 1.92 min\n",
      "Epoch: 004/015 | Batch 000/266 | Loss: 0.6978\n",
      "Epoch: 004/015 | Batch 050/266 | Loss: 0.6856\n",
      "Epoch: 004/015 | Batch 100/266 | Loss: 0.6926\n",
      "Epoch: 004/015 | Batch 150/266 | Loss: 0.6880\n",
      "Epoch: 004/015 | Batch 200/266 | Loss: 0.6897\n",
      "Epoch: 004/015 | Batch 250/266 | Loss: 0.6894\n",
      "training accuracy: 50.31%\n",
      "valid accuracy: 50.20%\n",
      "Time elapsed: 2.41 min\n",
      "Epoch: 005/015 | Batch 000/266 | Loss: 0.6878\n",
      "Epoch: 005/015 | Batch 050/266 | Loss: 0.6855\n",
      "Epoch: 005/015 | Batch 100/266 | Loss: 0.6884\n",
      "Epoch: 005/015 | Batch 150/266 | Loss: 0.6879\n",
      "Epoch: 005/015 | Batch 200/266 | Loss: 0.6890\n",
      "Epoch: 005/015 | Batch 250/266 | Loss: 0.6893\n",
      "training accuracy: 50.33%\n",
      "valid accuracy: 50.17%\n",
      "Time elapsed: 2.91 min\n",
      "Epoch: 006/015 | Batch 000/266 | Loss: 0.6881\n",
      "Epoch: 006/015 | Batch 050/266 | Loss: 0.6854\n",
      "Epoch: 006/015 | Batch 100/266 | Loss: 0.6883\n",
      "Epoch: 006/015 | Batch 150/266 | Loss: 0.6877\n",
      "Epoch: 006/015 | Batch 200/266 | Loss: 0.6889\n",
      "Epoch: 006/015 | Batch 250/266 | Loss: 0.6892\n",
      "training accuracy: 50.34%\n",
      "valid accuracy: 50.20%\n",
      "Time elapsed: 3.62 min\n",
      "Epoch: 007/015 | Batch 000/266 | Loss: 0.6875\n",
      "Epoch: 007/015 | Batch 050/266 | Loss: 0.6854\n",
      "Epoch: 007/015 | Batch 100/266 | Loss: 0.6883\n",
      "Epoch: 007/015 | Batch 150/266 | Loss: 0.6876\n",
      "Epoch: 007/015 | Batch 200/266 | Loss: 0.6889\n",
      "Epoch: 007/015 | Batch 250/266 | Loss: 0.6892\n",
      "training accuracy: 50.35%\n",
      "valid accuracy: 50.20%\n",
      "Time elapsed: 4.50 min\n",
      "Epoch: 008/015 | Batch 000/266 | Loss: 0.6874\n",
      "Epoch: 008/015 | Batch 050/266 | Loss: 0.6854\n",
      "Epoch: 008/015 | Batch 100/266 | Loss: 0.6883\n",
      "Epoch: 008/015 | Batch 150/266 | Loss: 0.6876\n",
      "Epoch: 008/015 | Batch 200/266 | Loss: 0.6889\n",
      "Epoch: 008/015 | Batch 250/266 | Loss: 0.6892\n",
      "training accuracy: 50.35%\n",
      "valid accuracy: 50.23%\n",
      "Time elapsed: 5.43 min\n",
      "Epoch: 009/015 | Batch 000/266 | Loss: 0.6874\n",
      "Epoch: 009/015 | Batch 050/266 | Loss: 0.6854\n",
      "Epoch: 009/015 | Batch 100/266 | Loss: 0.6883\n",
      "Epoch: 009/015 | Batch 150/266 | Loss: 0.6876\n",
      "Epoch: 009/015 | Batch 200/266 | Loss: 0.6889\n",
      "Epoch: 009/015 | Batch 250/266 | Loss: 0.6892\n",
      "training accuracy: 50.35%\n",
      "valid accuracy: 50.20%\n",
      "Time elapsed: 6.05 min\n",
      "Epoch: 010/015 | Batch 000/266 | Loss: 0.6874\n",
      "Epoch: 010/015 | Batch 050/266 | Loss: 0.6854\n",
      "Epoch: 010/015 | Batch 100/266 | Loss: 0.6884\n",
      "Epoch: 010/015 | Batch 150/266 | Loss: 0.6876\n",
      "Epoch: 010/015 | Batch 200/266 | Loss: 0.6889\n",
      "Epoch: 010/015 | Batch 250/266 | Loss: 0.6885\n",
      "training accuracy: 50.31%\n",
      "valid accuracy: 50.22%\n",
      "Time elapsed: 6.54 min\n",
      "Epoch: 011/015 | Batch 000/266 | Loss: 0.6866\n",
      "Epoch: 011/015 | Batch 050/266 | Loss: 0.6849\n",
      "Epoch: 011/015 | Batch 100/266 | Loss: 0.6963\n",
      "Epoch: 011/015 | Batch 150/266 | Loss: 0.6876\n",
      "Epoch: 011/015 | Batch 200/266 | Loss: 0.6983\n",
      "Epoch: 011/015 | Batch 250/266 | Loss: 0.6892\n",
      "training accuracy: 50.34%\n",
      "valid accuracy: 50.18%\n",
      "Time elapsed: 7.03 min\n",
      "Epoch: 012/015 | Batch 000/266 | Loss: 0.6889\n",
      "Epoch: 012/015 | Batch 050/266 | Loss: 0.6854\n",
      "Epoch: 012/015 | Batch 100/266 | Loss: 0.6883\n",
      "Epoch: 012/015 | Batch 150/266 | Loss: 0.6876\n",
      "Epoch: 012/015 | Batch 200/266 | Loss: 0.6889\n",
      "Epoch: 012/015 | Batch 250/266 | Loss: 0.6773\n",
      "training accuracy: 60.61%\n",
      "valid accuracy: 59.75%\n",
      "Time elapsed: 7.52 min\n",
      "Epoch: 013/015 | Batch 000/266 | Loss: 0.6732\n",
      "Epoch: 013/015 | Batch 050/266 | Loss: 0.6629\n",
      "Epoch: 013/015 | Batch 100/266 | Loss: 0.6658\n",
      "Epoch: 013/015 | Batch 150/266 | Loss: 0.5675\n",
      "Epoch: 013/015 | Batch 200/266 | Loss: 0.4676\n",
      "Epoch: 013/015 | Batch 250/266 | Loss: 0.4690\n",
      "training accuracy: 78.62%\n",
      "valid accuracy: 75.20%\n",
      "Time elapsed: 8.01 min\n",
      "Epoch: 014/015 | Batch 000/266 | Loss: 0.5032\n",
      "Epoch: 014/015 | Batch 050/266 | Loss: 0.4369\n",
      "Epoch: 014/015 | Batch 100/266 | Loss: 0.4936\n",
      "Epoch: 014/015 | Batch 150/266 | Loss: 0.3867\n",
      "Epoch: 014/015 | Batch 200/266 | Loss: 0.3653\n",
      "Epoch: 014/015 | Batch 250/266 | Loss: 0.3117\n",
      "training accuracy: 88.19%\n",
      "valid accuracy: 84.15%\n",
      "Time elapsed: 8.49 min\n",
      "Epoch: 015/015 | Batch 000/266 | Loss: 0.3814\n",
      "Epoch: 015/015 | Batch 050/266 | Loss: 0.3545\n",
      "Epoch: 015/015 | Batch 100/266 | Loss: 0.3451\n",
      "Epoch: 015/015 | Batch 150/266 | Loss: 0.2200\n",
      "Epoch: 015/015 | Batch 200/266 | Loss: 0.1906\n",
      "Epoch: 015/015 | Batch 250/266 | Loss: 0.2948\n",
      "training accuracy: 91.17%\n",
      "valid accuracy: 85.55%\n",
      "Time elapsed: 8.99 min\n",
      "Total Training Time: 8.99 min\n",
      "Test accuracy: 85.35%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "# for epoch in range(2):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        text = batch_data[0].to(DEVICE)\n",
    "        labels = batch_data[1].to(DEVICE)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%'\n",
    "            )\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our vocab and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_vocab, os.path.join(DATA_DIRECTORY, 'text_vocab.pkl'))\n",
    "torch.save(label_vocab, os.path.join(DATA_DIRECTORY, 'label_vocab.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, os.path.join(DATA_DIRECTORY, 'my_model.pkl'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_text_vocab = torch.load(os.path.join(DATA_DIRECTORY, 'text_vocab.pkl'))\n",
    "loaded_label_vocab = torch.load(os.path.join(DATA_DIRECTORY, 'label_vocab.pkl'))\n",
    "\n",
    "loaded_model = torch.load(os.path.join(DATA_DIRECTORY, 'my_model.pkl'), map_location=DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to produce model predictions from a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jt55pscgFdKZ"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval() # switch model to eval mode\n",
    "\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)] # tokenize text\n",
    "    indexed = loaded_text_vocab(tokenized) # get labels\n",
    "\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "    tensor = tensor.unsqueeze(1) # add an extra dimension to act as if it is multiple sentences\n",
    "    \n",
    "    prediction = torch.nn.functional.softmax(model(tensor), dim=1)\n",
    "    prediction = prediction[0] # get rid of unnessary dimension\n",
    "\n",
    "    # get dictionary from labels to probabilities\n",
    "    prediction_dict = {}\n",
    "    for i, prob in enumerate(prediction):\n",
    "        label = loaded_label_vocab.get_itos()[i]\n",
    "        prediction_dict[label] = prob.item()\n",
    "\n",
    "    # convert labels to something readable\n",
    "    label_to_readable = {\n",
    "        '0': 'negative',\n",
    "        '1': 'positive',\n",
    "    }\n",
    "    prediction_dict = {label_to_readable[k]: v for k,v in prediction_dict.items()}\n",
    "\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 0.9998838901519775, 'negative': 0.000116095005068928}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(loaded_model, \"This is such an awesome movie, I really love it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 0.08141873776912689, 'negative': 0.9185812473297119}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(loaded_model, \"I really hate this movie. It is really bad and sucks!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_lstm_packed_imdb.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c319c4c1f62fbd0e701fc1771b1a38700bd3fb463794e8a3026ac9899a51c14b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
